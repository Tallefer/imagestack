==lahbpcg==
---
_This page was auto-generated from the ImageStack -help operator. Direct edits to it will be lost. Use the comments below to discuss this operation._
---
{{{
ImageStack -help lahbpcg

-lahbpcg takes six images from the stack and treats them as a target output, x
gradient, and y gradient, and then the respective weights for each term. The
weights may be single-channel or have the same number of channels as the target
images. It then attempts to solve for the image which best achieves that target
ouput and those target gradients in the weighted-least-squares sense using a
preconditioned weighted least squares solver. This technique is useful for a
variety of problems with constraints expressed in the gradient domain,
including Poisson solves, making a sparse labelling dense, and other
gradient-domain techniques.

This operator takes two arguments. The first specifies the maximum number of
iterations, and the second specifies the error required for convergence

The following example takes a sparse labelling of an image im.jpg, and expands
it to be dense in a manner that respects the boundaries of the image. The
gradient weights used are uniform.
Usage: ImageStack -load sparse_labels.tmp \
                  -load im.jpg -gradient x \
                  -load im.jpg -gradient y \
                  -load sparse_weights.tmp \
                  -push -offset 1 \
                  -dup \
                  -lahbpcg 200 0.001 -save out.png
}}}
----
_This page was auto-generated from the !ImageStack -help operator. Direct edits to it will be lost. Use the comments below to discuss this operation._
----
{{{
ImageStack -help lahbpcg

-lahbpcg takes six images from the stack and treats them as a target output, x
gradient, and y gradient, and then the respective weights for each term. The
weights may be single-channel or have the same number of channels as the target
images. It then attempts to solve for the image which best achieves that target
ouput and those target gradients in the weighted-least-squares sense using a
preconditioned weighted least squares solver. This technique is useful for a
variety of problems with constraints expressed in the gradient domain,
including Poisson solves, making a sparse labelling dense, and other
gradient-domain techniques.

This operator takes two arguments. The first specifies the maximum number of
iterations, and the second specifies the error required for convergence

The following example takes a sparse labelling of an image im.jpg, and expands
it to be dense in a manner that respects the boundaries of the image. The
gradient weights used are uniform.
Usage: ImageStack -load sparse_labels.tmp \
                  -load im.jpg -gradient x \
                  -load im.jpg -gradient y \
                  -load sparse_weights.tmp \
                  -push -offset 1 \
                  -dup \
                  -lahbpcg 200 0.001 -save out.png
}}}
